{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdbe70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подключим необходимые библиотеки\n",
    "from typing import Union, Optional, Tuple, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.lm.vocabulary import Vocabulary\n",
    "from sklearn.model_selection import train_test_split as TTsplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import trange\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e72ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# По имени файла формирует признаки, записывая их в переменные\n",
    "def split_twitter_data(filename: str,\n",
    "                       target: bool = True) -> Tuple[List, List, List, Optional[List]]:\n",
    "    df = pd.read_csv(filename)\n",
    "    keywords, locations, texts = list(df.iloc[:, 1]), list(df.iloc[:, 2]), list(df.iloc[:, 3])\n",
    "    targets = None\n",
    "    if target:\n",
    "        targets = list(df.iloc[:, 4])\n",
    "    return keywords, locations, texts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0dcbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Производит токенизацию списка строк выбранным токенизатором\n",
    "def tokenize(strings: List[Optional[str]],\n",
    "             tokenizer, callable_ : bool = False) -> List[List[str]]:\n",
    "    result = []\n",
    "    for s in strings:\n",
    "        if s is np.nan:\n",
    "            result.append('')\n",
    "        else:\n",
    "            result.append(s)\n",
    "    result = [tokenizer(s) for s in result] if callable_ else [tokenizer.tokenize(s) for s in result]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a8a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обрезает слишком длинные токенизированные строки, удлинняяет короткие\n",
    "# В результате каждая токенизированная строка состоит из одинакового количества токенов\n",
    "def to_one_len(tokenized_strings: List[List[str]]) -> List[List[str]]:\n",
    "    result = []\n",
    "    lens = np.array([len(s) for s in tokenized_strings])\n",
    "    tokenized_string_max_len = int(np.ceil(np.quantile(lens, 0.95)))\n",
    "    result = [s[:tokenized_string_max_len] for s in tokenized_strings]\n",
    "    for s in result:\n",
    "        if len(s) < tokenized_string_max_len:\n",
    "            s.extend(['<UNK>'] * (tokenized_string_max_len - len(s)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e106d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# По списку токенизированных строк получает словарь\n",
    "def get_vocab(*features, unk_cutoff=1) -> Vocabulary:\n",
    "    tokens = []\n",
    "    for feature in features:\n",
    "        tokens.extend([token for observation in feature for token in observation])\n",
    "    tokens = np.random.permutation(tokens)\n",
    "    vocab = Vocabulary(tokens, unk_cutoff)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12fe6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переводит каждый токен в его индекс в словаре\n",
    "def to_idx(vocab, *features) -> List[np.array]:\n",
    "    result = []\n",
    "    for feature in features:\n",
    "        result.append(\n",
    "            np.array(\n",
    "                [[vocab[token] for token in observation] for observation in feature],\n",
    "                dtype=np.int32\n",
    "            )\n",
    "        )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e7182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружает данные из файла с именем filename\n",
    "# После вызова функции получаем данные, готовые для обучения модели\n",
    "def twitter_data_preprocessing(filename: str,\n",
    "                               vocab: Optional[Vocabulary] = None,\n",
    "                               target: bool = True,\n",
    "                               unk_cutoff: int = 1,\n",
    "                               test_size: float = 0.2,\n",
    "                              ) -> Tuple[Vocabulary, torch.tensor, Optional[torch.tensor]]:\n",
    "    # Чтение и разбиение файла на признаки\n",
    "    keywords, locations, texts, targets = split_twitter_data(filename, target)\n",
    "    if targets is not None:\n",
    "        targets = np.array(targets, dtype=np.int64)\n",
    "    \n",
    "    # Задание токенизаторов\n",
    "    regular_tokenizer = nltk.tokenize.wordpunct_tokenize\n",
    "    tweet_tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True,\n",
    "                                                   strip_handles=True, match_phone_numbers=False)\n",
    "    \n",
    "    # Токенизация признаков\n",
    "    keywords = tokenize(keywords, regular_tokenizer, callable_=True)\n",
    "    locations = tokenize(locations, regular_tokenizer, callable_=True)\n",
    "    texts = tokenize(texts, tweet_tokenizer, callable_=False)\n",
    "    \n",
    "    # Разбиение на train и test\n",
    "    if test_size != 0 and target:\n",
    "        k_train, k_test, l_train, l_test, t_train, t_test, tg_train, tg_test = TTsplit(keywords,\n",
    "                                                                                       locations,\n",
    "                                                                                       texts,\n",
    "                                                                                       targets,\n",
    "                                                                                       test_size=test_size)\n",
    "    elif test_size != 0:\n",
    "        k_train, k_test, l_train, l_test, t_train, t_test = TTsplit(keywords,\n",
    "                                                                    locations,\n",
    "                                                                    texts,\n",
    "                                                                    test_size=test_size)\n",
    "    \n",
    "    # Приведение размерности признаков к одной длине\n",
    "    if test_size == 0:\n",
    "        keywords = to_one_len(keywords)\n",
    "        locations = to_one_len(locations)\n",
    "        texts = to_one_len(texts)\n",
    "    else:\n",
    "        k_train, k_test = to_one_len(k_train), to_one_len(k_test)\n",
    "        l_train, l_test = to_one_len(l_train), to_one_len(l_test)\n",
    "        t_train, t_test = to_one_len(t_train), to_one_len(t_test)\n",
    "    \n",
    "    # Получение словаря\n",
    "    if vocab is None:\n",
    "        if test_size == 0:\n",
    "            vocab = get_vocab(keywords, locations, texts, unk_cutoff=unk_cutoff)\n",
    "        else:\n",
    "            vocab = get_vocab(k_train, l_train, t_train, unk_cutoff=unk_cutoff)\n",
    "    \n",
    "    # Преобразование признаков в векторы (токен -> его индекс в словаре vocab)\n",
    "    if test_size == 0:\n",
    "        keywords, locations, texts = to_idx(vocab, keywords, locations, texts)\n",
    "    else:\n",
    "        k_train, l_train, t_train = to_idx(vocab, k_train, l_train, t_train)\n",
    "        k_test, l_test, t_test = to_idx(vocab, k_test, l_test, t_test) \n",
    "    \n",
    "    # Конкатенация признаков\n",
    "    if test_size == 0:\n",
    "        data = torch.hstack((keywords, locations, texts))\n",
    "    else:\n",
    "        data_train = np.hstack((k_train, l_train, t_train))\n",
    "        data_test = np.hstack((k_test, l_test, t_test))\n",
    "    \n",
    "    if test_size == 0:\n",
    "        return vocab, data, targets\n",
    "    \n",
    "    if target:\n",
    "        return vocab, data_train, data_test, tg_train, tg_test\n",
    "    \n",
    "    return vocab, data_train, data_test, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da749ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.25 s\n",
      "Wall time: 1.25 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3430"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Предобработка данных из файла train.csv, перенос на видеопамять\n",
    "vocab, X_train, X_test, y_train, y_test = twitter_data_preprocessing('train.csv',\n",
    "                                                                     target=True, unk_cutoff=4)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf7159",
   "metadata": {},
   "source": [
    "Решим задачу с помощью наивного байесовского классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61d221f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on train data after training: 0.5801313628899836\n",
      "Model accuracy on test data after training: 0.5666447800393959\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 8.14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = MultinomialNB(class_prior=np.array([0.6, 0.4]))\n",
    "model.fit(X_train, y_train)\n",
    "print('Model accuracy on train data after training: {}'.format(model.score(X_train, y_train)))\n",
    "print('Model accuracy on test data after training: {}'.format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ccfdd6",
   "metadata": {},
   "source": [
    "Теперь решим задачу с помощью нейросетевой модели LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7520f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описание модели нейронной сети LSTM для классификации\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size//4)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.linear2 = nn.Linear(hidden_size//4, hidden_size//8)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.linear3 = nn.Linear(hidden_size//8, 2)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        emb_out = self.emb(inp)\n",
    "        lstm_out, _ = self.lstm(emb_out)\n",
    "        lin1_out = self.linear1(self.dropout1(lstm_out[:, -1]))\n",
    "        lin2_out = self.linear2(self.dropout2(self.relu2(lin1_out)))\n",
    "        lin3_out = self.linear3(self.dropout3(self.relu3(lin2_out)))\n",
    "        return lin3_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35c7b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описание процесса тренировки нейронной сети\n",
    "def train(model, loss_function, X, y, epochs, optimizer, **params):\n",
    "    print('Training: ')\n",
    "    optimizer_ = optimizer(model.parameters(), **params)\n",
    "    for _ in trange(epochs):\n",
    "        out = model(X)\n",
    "        loss = loss_function(out, y)\n",
    "        optimizer_.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b51059de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, X, y):\n",
    "    model.eval()\n",
    "    y_pred = model(X).detach().argmax(axis=1)\n",
    "    score = (y_pred == y).sum() / len(y)\n",
    "    model.train()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86e2ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = torch.tensor(X_train, device='cuda'), torch.tensor(X_test, device='cuda')\n",
    "y_train, y_test = torch.tensor(y_train, device='cuda'), torch.tensor(y_test, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0370fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание модели и функции потерь\n",
    "model = LSTM(len(vocab), 500, 500).cuda()\n",
    "loss_function = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████████████████████████████████▉                  | 772/1000 [03:13<00:57,  3.96it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Тренировка модели, проверка её качества на тренировочных данных после обучения\n",
    "train(model, loss_function, X_train, y_train, epochs=1000, optimizer=Adam, lr=1e-3)\n",
    "print('Model accuracy on train data after training: {}'.format(accuracy(model, X_train, y_train)))\n",
    "print('Model accuracy on test data after training: {}'.format(accuracy(model, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Часть кода для формирования таблицы на kaggle\n",
    "\n",
    "\"\"\"\n",
    "%%time\n",
    "_, X_test, _ = twitter_data_preprocessing('test.csv', vocab, target=False, test_size=0)\n",
    "indices = np.array(pd.read_csv('test.csv').iloc[:, 0], dtype=np.int32)\n",
    "\n",
    "# Считает предсказания модели и записывает в таблицу\n",
    "def predict_and_save(model, data, indices, batch_size=40, filename='sample_submission.csv'):\n",
    "    data_ = data.cuda()\n",
    "    out = model(data_).argmax(axis=1).cpu().detach().numpy().astype(np.int32)\n",
    "    table = pd.DataFrame({'id': indices, 'target': np.hstack(outs)})\n",
    "    table.to_csv(filename, index=False)\n",
    "\n",
    "predict_and_save(model, X_test, indices)\n",
    "\"\"\";"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
